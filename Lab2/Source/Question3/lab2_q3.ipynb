{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2_q3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpLw5Z92ALUL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7be9b6d0-ddac-4793-cdc3-14531ec9c750"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from numpy import array"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXstHxzKAQaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(train_file, test_file=None):\n",
        "    if test_file == None:\n",
        "        frame = pd.read_csv(train_file)\n",
        "        data = frame.values\n",
        "        np.random.shuffle(data)\n",
        "        return data\n",
        "    else:\n",
        "        train_frame = pd.read_csv(train_file)\n",
        "        test_frame = pd.read_csv(test_file)\n",
        "\n",
        "        train_data = train_frame.values\n",
        "        test_data = test_frame.values\n",
        "        np.random.shuffle(train_data)\n",
        "        np.random.shuffle(test_data)\n",
        "\n",
        "        return train_data, test_data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHvRNcdnAhOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_training_testing_sets(train_file, test_file=None):\n",
        "    if test_file == None:\n",
        "        data = get_data(train_file)\n",
        "        train_data, test_data = train_test_split(data)\n",
        "    else:\n",
        "\n",
        "        train_data, test_data = get_data(train_file, test_file)\n",
        "\n",
        "    X_train = train_data[:, 1]\n",
        "    Y_train = train_data[:, 0]\n",
        "    X_test = test_data[:, 1]\n",
        "    Y_test = test_data[:, 0]\n",
        "\n",
        "    print(X_train.shape, X_test.shape)\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFA7F4l7ApGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoGC2Sj4AtwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(lines):\n",
        "    return max([len(sentence.split()) for sentence in lines])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90tihbeSA1gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_text(tokenizer, lines, length):\n",
        "    encoded = tokenizer.texts_to_sequences(lines)\n",
        "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "    return padded\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1tHQ-1UA3ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model(length, vocab_size, channels, kernel_size):\n",
        "    inputs = {}\n",
        "    embedding = {}\n",
        "    conv = {}\n",
        "    drop = {}\n",
        "    pool = {}\n",
        "    flat = {}\n",
        "    for channel in range(1, channels + 1):\n",
        "        inputs[channel] = Input(shape=(length,))\n",
        "        embedding[channel] = Embedding(vocab_size, 100)(inputs[channel])\n",
        "        conv[channel] = Conv1D(filters=32, kernel_size=kernel_size[channel], activation='relu')(embedding[channel])\n",
        "        drop[channel] = Dropout(0.5)(conv[channel])\n",
        "        pool[channel] = MaxPooling1D(pool_size=2)(drop[channel])\n",
        "        flat[channel] = Flatten()(pool[channel])\n",
        "    merged = concatenate(list(flat.values()))\n",
        "    dense = Dense(10, activation='relu')(merged)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "    model = Model(list(inputs.values()), outputs=outputs)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    print(model.summary())\n",
        "    plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf3XTz9GA-LL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb50311d-e29b-46f2-a9b1-507dc2e710e1"
      },
      "source": [
        "X_train, Y_train, X_test, Y_test = get_training_testing_sets('/content/SPAM text message 20170820 - Data.csv')\n",
        "for i in range(Y_train.shape[0]):\n",
        "    Y_train[i] = (Y_train[i] == 'spam')\n",
        "\n",
        "for i in range(Y_test.shape[0]):\n",
        "    Y_test[i] = (Y_test[i] == 'spam')\n",
        "\n",
        "tokenizer = get_tokenizer(X_train)\n",
        "length = max_length(X_train)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "X_train = encode_text(tokenizer, X_train, length)\n",
        "model = define_model(length, vocab_size, 3, {1: 8, 2: 6, 3: 4})\n",
        "model.fit([X_train, X_train, X_train], array(Y_train), epochs=10, batch_size=128)\n",
        "\n",
        "tokenizer = get_tokenizer(X_test)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "X_test = encode_text(tokenizer, X_test, length)\n",
        "loss, acc = model.evaluate([X_test, X_test, X_test], array(Y_test), verbose=0)\n",
        "\n",
        "print(\"Accuracy:\",acc*100)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4179,) (1393,)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 171)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 171)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, 171)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 171, 100)     782300      input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 171, 100)     782300      input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 171, 100)     782300      input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 164, 32)      25632       embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 166, 32)      19232       embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 168, 32)      12832       embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 164, 32)      0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 166, 32)      0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 168, 32)      0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 82, 32)       0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 83, 32)       0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 84, 32)       0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 2624)         0           max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 2656)         0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 2688)         0           max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 7968)         0           flatten_4[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 10)           79690       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            11          dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,484,297\n",
            "Trainable params: 2,484,297\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "4179/4179 [==============================] - 13s 3ms/step - loss: 0.3377 - acc: 0.8643\n",
            "Epoch 2/10\n",
            "4179/4179 [==============================] - 12s 3ms/step - loss: 0.1532 - acc: 0.9349\n",
            "Epoch 3/10\n",
            "4179/4179 [==============================] - 12s 3ms/step - loss: 0.1128 - acc: 0.9902\n",
            "Epoch 4/10\n",
            "4179/4179 [==============================] - 12s 3ms/step - loss: 0.1011 - acc: 0.9933\n",
            "Epoch 5/10\n",
            "4179/4179 [==============================] - 12s 3ms/step - loss: 0.0947 - acc: 0.9969\n",
            "Epoch 6/10\n",
            "4179/4179 [==============================] - 12s 3ms/step - loss: 0.0909 - acc: 0.9974\n",
            "Epoch 7/10\n",
            "4179/4179 [==============================] - 12s 3ms/step - loss: 0.0888 - acc: 0.9971\n",
            "Epoch 8/10\n",
            "4179/4179 [==============================] - 12s 3ms/step - loss: 0.0846 - acc: 0.9981\n",
            "Epoch 9/10\n",
            "4179/4179 [==============================] - 12s 3ms/step - loss: 0.0818 - acc: 0.9988\n",
            "Epoch 10/10\n",
            "4179/4179 [==============================] - 12s 3ms/step - loss: 0.0790 - acc: 0.9990\n",
            "Accuracy: 85.49892320875975\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}